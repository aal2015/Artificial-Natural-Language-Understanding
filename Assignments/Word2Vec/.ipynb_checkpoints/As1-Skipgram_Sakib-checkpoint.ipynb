{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27651909-60bc-48f8-9910-f9edec4cac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e55ef-93a6-42da-a1cf-90b7f23be6d1",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e92b3da-775f-426a-8cdc-00f73fdf57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 documents, each having 30 words\n",
    "corpus = [\"throughout history governments from around the world have undergone various modifications with the goal of improving capacity, efficiency, and citizen services. with the creation of cutting-edge technology, this progress was\", \n",
    "          \"one of the technologies that has shown to be extremely important for our daily life in all facets of the digital society is the internet of things (iot). iot is\",\n",
    "          \"regarded as the foundation of smart cities, just as smart cities serve as the foundation of smart governments. this study explores the applications of iot in smart government along with\",\n",
    "          \"by considering smart government as an extension of e-government many countries have started to invest in this domain. dubai, australia, singapore and moldova have already taken some initiatives and found \",\n",
    "          \"significant result of smart government (alonaizi & manuel, 2021). the phrase  indicates the update to e-government that enables users to quickly get government services using smart technologies by \",\n",
    "          \"interacting  with them in new ways as a result of the popularity of mobile applications, social media, and other smart devices (algebri et al., 2018). the cornerstone of creating public\",\n",
    "          \"the relationship via which a government extends its value chain to people. the basis for the working definition utilized in this study is moore's thesis that public value incorporates\",\n",
    "          \"real-time communication between individuals and the public sectors is a key indicator of how valuable e-government services are to the general public (karunasena & deng, 2012). socio cultural, economical, financial\",\n",
    "          \"the word iot refers to a collection of real-world items that have sensors built in and are networked together to provide useful data. definitions of iot have evolved in line\",\n",
    "          \"the bulk of iot efforts have been implemented in the commercial sectors, however researches have shown that iot deployment in the public sector has significantly increased\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702f98b7-4c65-4002-803d-443a21e7e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sent.split(\" \") for sent in corpus]\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2982f9-ca91-47ca-abfd-1b9402d9ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word sequences and unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(corpus)))\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec45041e-3b41-4fef-bcb1-6b99e586f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalization\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "#print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80fa5cce-61d7-4695-b9b3-0e4be6b3ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "#vocab size\n",
    "voc_size = len(vocab)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a5b0b8-b19d-4efb-9bf3-ac884b94c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append UNK\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02de58e7-1d0a-4616-872c-b2cf63b421c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index['<UNK>'] = 177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8052c755-a56c-4f17-9fc1-5e28c6464704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just in case we need to use\n",
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b09d29-3f38-4873-94ab-5909809b7652",
   "metadata": {},
   "source": [
    "# 2. Prepare Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d1f7cda8-c2a2-412f-ad39-2008586bc3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['throughout', 'history', 'governments', 'from', 'around', 'the', 'world', 'have', 'undergone', 'various', 'modifications', 'with', 'the', 'goal', 'of', 'improving', 'capacity,', 'efficiency,', 'and', 'citizen', 'services.', 'with', 'the', 'creation', 'of', 'cutting-edge', 'technology,', 'this', 'progress', 'was']\n",
      "['one', 'of', 'the', 'technologies', 'that', 'has', 'shown', 'to', 'be', 'extremely', 'important', 'for', 'our', 'daily', 'life', 'in', 'all', 'facets', 'of', 'the', 'digital', 'society', 'is', 'the', 'internet', 'of', 'things', '(iot).', 'iot', 'is']\n",
      "['regarded', 'as', 'the', 'foundation', 'of', 'smart', 'cities,', 'just', 'as', 'smart', 'cities', 'serve', 'as', 'the', 'foundation', 'of', 'smart', 'governments.', 'this', 'study', 'explores', 'the', 'applications', 'of', 'iot', 'in', 'smart', 'government', 'along', 'with']\n",
      "['by', 'considering', 'smart', 'government', 'as', 'an', 'extension', 'of', 'e-government', 'many', 'countries', 'have', 'started', 'to', 'invest', 'in', 'this', 'domain.', 'dubai,', 'australia,', 'singapore', 'and', 'moldova', 'have', 'already', 'taken', 'some', 'initiatives', 'and', 'found', '']\n",
      "['significant', 'result', 'of', 'smart', 'government', '(alonaizi', '&', 'manuel,', '2021).', 'the', 'phrase', '', 'indicates', 'the', 'update', 'to', 'e-government', 'that', 'enables', 'users', 'to', 'quickly', 'get', 'government', 'services', 'using', 'smart', 'technologies', 'by', '']\n",
      "['interacting', '', 'with', 'them', 'in', 'new', 'ways', 'as', 'a', 'result', 'of', 'the', 'popularity', 'of', 'mobile', 'applications,', 'social', 'media,', 'and', 'other', 'smart', 'devices', '(algebri', 'et', 'al.,', '2018).', 'the', 'cornerstone', 'of', 'creating', 'public']\n",
      "['the', 'relationship', 'via', 'which', 'a', 'government', 'extends', 'its', 'value', 'chain', 'to', 'people.', 'the', 'basis', 'for', 'the', 'working', 'definition', 'utilized', 'in', 'this', 'study', 'is', \"moore's\", 'thesis', 'that', 'public', 'value', 'incorporates']\n",
      "['real-time', 'communication', 'between', 'individuals', 'and', 'the', 'public', 'sectors', 'is', 'a', 'key', 'indicator', 'of', 'how', 'valuable', 'e-government', 'services', 'are', 'to', 'the', 'general', 'public', '(karunasena', '&', 'deng,', '2012).', 'socio', 'cultural,', 'economical,', 'financial']\n",
      "['the', 'word', 'iot', 'refers', 'to', 'a', 'collection', 'of', 'real-world', 'items', 'that', 'have', 'sensors', 'built', 'in', 'and', 'are', 'networked', 'together', 'to', 'provide', 'useful', 'data.', 'definitions', 'of', 'iot', 'have', 'evolved', 'in', 'line']\n",
      "['the', 'bulk', 'of', 'iot', 'efforts', 'have', 'been', 'implemented', 'in', 'the', 'commercial', 'sectors,', 'however', 'researches', 'have', 'shown', 'that', 'iot', 'deployment', 'in', 'the', 'public', 'sector', 'has', 'significantly', 'increased']\n"
     ]
    }
   ],
   "source": [
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fe9533dd-b173-47eb-b264-24ae9bc11d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2index\n",
    "skipgrams = []\n",
    "\n",
    "#for each corpus\n",
    "for sent in corpus:\n",
    "    for i in range(2, len(sent) - 2): \n",
    "        center_word = sent[i]\n",
    "        outside_words = [sent[i-2], sent[i-1], sent[i+1], sent[i+2]]  #window_size = 2\n",
    "        for o in outside_words:\n",
    "            skipgrams.append([center_word, o])\n",
    "\n",
    "#skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad56728-33b5-4d02-9762-c5365ec6d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence):\n",
    "    \n",
    "    # Make skip gram of window size 2\n",
    "    skip_grams = []\n",
    "    # loop each word sequence\n",
    "    for sent in corpus:\n",
    "        for i in range(2, len(sent) - 2):\n",
    "            target = word2index[sent[i]]\n",
    "            context = [word2index[sent[i - 2]], word2index[sent[i - 1]], word2index[sent[i + 1]], word2index[sent[i + 2]]] # window size 2\n",
    "            for w in context:\n",
    "                skip_grams.append([target, w])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b3d9a-728c-4467-93cc-27b948b81519",
   "metadata": {},
   "source": [
    "## Testing the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78efb221-cc77-4d9b-8ef3-eb154a173a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[102]\n",
      " [130]]\n",
      "Target:  [[ 11]\n",
      " [144]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch = random_batch(batch_size, corpus)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff6983-d9d5-4970-a0f7-d4ffbd624f9c",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9a64a1-1c90-4298-9635-384abcb81f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_words, target_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)# [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        all_embeds    = self.embedding_v(all_vocabs) #   [batch_size, voc_size, emb_size]\n",
    "        # print(all_embeds.shape)\n",
    "        \n",
    "        scores      = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, voc_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, voc_size, 1] = [batch_size, voc_size]\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        # scalar (loss must be scalar)    \n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2d12b-59b7-4f9a-bc37-217df6cd03f5",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6aea53e-796d-4a83-9faf-1fccd82703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 2 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "model          = Skipgram(len(vocab), embedding_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aab2726d-1ee2-4d8a-b14d-2c5c75c12768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_v): Embedding(178, 2)\n",
       "  (embedding_u): Embedding(178, 2)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42073a28-71d4-413f-9d53-033e4608b47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 178])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#use for the normalized term in the probability calculation\n",
    "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab))  # [batch_size, voc_size]\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "056d066e-9710-4447-84c5-9481d4bcae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fee73dc1-da33-4120-bcc6-9e095392de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a1e5318-a905-4c60-b81f-e01336fe0a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 5.877048 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 4.822315 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 5.309200 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 5.415352 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 4.698609 | time: 0m 0s\n",
      "Epoch: 6000 | cost: 5.542280 | time: 0m 0s\n",
      "Epoch: 7000 | cost: 4.896063 | time: 0m 0s\n",
      "Epoch: 8000 | cost: 5.807316 | time: 0m 0s\n",
      "Epoch: 9000 | cost: 0.938382 | time: 0m 0s\n",
      "Epoch: 10000 | cost: 4.497680 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, corpus)\n",
    "    input_batch  = torch.LongTensor(input_batch)  \n",
    "    target_batch = torch.LongTensor(target_batch) \n",
    "    # print(input_batch.shape)\n",
    "    # print(target_batch.shape)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79ba9e-960b-46f6-aa6e-f61d6c568c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3050d7c0-3b84-4418-a410-c0e89a2470cc",
   "metadata": {},
   "source": [
    "# Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0407e-ccfd-427e-9949-6118fe7cd541",
   "metadata": {},
   "source": [
    "## Unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31e2aacf-de9a-4b09-8d14-f509804fce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 0.001\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum([c for w, c in word_count.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9636808-9d69-40fe-bc12-ba08c3696051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db7623ec-9280-461f-86e4-cf5d3dc9b255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_total_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b2b21b4-3dbc-4421-83aa-3aba5b91c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for vo in vocab:\n",
    "    unigram_table.extend([vo] * int(((word_count[vo]/num_total_words)**0.75)/Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "946bac8f-0ade-4345-bf30-0de0e150b60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'': 39,\n",
       "         '2018).': 13,\n",
       "         'manuel,': 13,\n",
       "         'cornerstone': 13,\n",
       "         'foundation': 23,\n",
       "         'governments.': 13,\n",
       "         'commercial': 13,\n",
       "         'provide': 13,\n",
       "         'society': 13,\n",
       "         'considering': 13,\n",
       "         'governments': 13,\n",
       "         'have': 60,\n",
       "         'invest': 13,\n",
       "         'new': 13,\n",
       "         '(algebri': 13,\n",
       "         '2012).': 13,\n",
       "         'financial': 13,\n",
       "         'individuals': 13,\n",
       "         'definitions': 13,\n",
       "         'progress': 13,\n",
       "         'shown': 23,\n",
       "         'general': 13,\n",
       "         'key': 13,\n",
       "         'deng,': 13,\n",
       "         'that': 46,\n",
       "         'around': 13,\n",
       "         'socio': 13,\n",
       "         '&': 23,\n",
       "         'a': 39,\n",
       "         'result': 23,\n",
       "         'researches': 13,\n",
       "         'built': 13,\n",
       "         'quickly': 13,\n",
       "         'sectors,': 13,\n",
       "         'how': 13,\n",
       "         'been': 13,\n",
       "         'things': 13,\n",
       "         'is': 39,\n",
       "         'moldova': 13,\n",
       "         'found': 13,\n",
       "         'initiatives': 13,\n",
       "         'refers': 13,\n",
       "         'government': 46,\n",
       "         'daily': 13,\n",
       "         'modifications': 13,\n",
       "         'study': 23,\n",
       "         'other': 13,\n",
       "         'sector': 13,\n",
       "         'using': 13,\n",
       "         'all': 13,\n",
       "         'digital': 13,\n",
       "         'our': 13,\n",
       "         'undergone': 13,\n",
       "         'items': 13,\n",
       "         'sensors': 13,\n",
       "         '(karunasena': 13,\n",
       "         'evolved': 13,\n",
       "         'countries': 13,\n",
       "         'services.': 13,\n",
       "         'efforts': 13,\n",
       "         'thesis': 13,\n",
       "         'from': 13,\n",
       "         'singapore': 13,\n",
       "         'incorporates': 13,\n",
       "         'economical,': 13,\n",
       "         'via': 13,\n",
       "         'applications': 13,\n",
       "         'are': 23,\n",
       "         'citizen': 13,\n",
       "         'chain': 13,\n",
       "         'creating': 13,\n",
       "         'this': 39,\n",
       "         'relationship': 13,\n",
       "         'utilized': 13,\n",
       "         'important': 13,\n",
       "         'e-government': 31,\n",
       "         'together': 13,\n",
       "         'media,': 13,\n",
       "         '(alonaizi': 13,\n",
       "         'improving': 13,\n",
       "         'extension': 13,\n",
       "         'of': 117,\n",
       "         'capacity,': 13,\n",
       "         'some': 13,\n",
       "         'social': 13,\n",
       "         'sectors': 13,\n",
       "         'technology,': 13,\n",
       "         'enables': 13,\n",
       "         'domain.': 13,\n",
       "         'one': 13,\n",
       "         'history': 13,\n",
       "         'them': 13,\n",
       "         'phrase': 13,\n",
       "         'networked': 13,\n",
       "         'started': 13,\n",
       "         'useful': 13,\n",
       "         'interacting': 13,\n",
       "         'be': 13,\n",
       "         'by': 23,\n",
       "         'just': 13,\n",
       "         'extends': 13,\n",
       "         'life': 13,\n",
       "         'iot': 53,\n",
       "         'et': 13,\n",
       "         'indicates': 13,\n",
       "         'however': 13,\n",
       "         'public': 46,\n",
       "         'cities': 13,\n",
       "         'word': 13,\n",
       "         \"moore's\": 13,\n",
       "         'communication': 13,\n",
       "         'serve': 13,\n",
       "         'technologies': 23,\n",
       "         'to': 66,\n",
       "         'efficiency,': 13,\n",
       "         '(iot).': 13,\n",
       "         'cities,': 13,\n",
       "         'explores': 13,\n",
       "         'cutting-edge': 13,\n",
       "         'popularity': 13,\n",
       "         'goal': 13,\n",
       "         'creation': 13,\n",
       "         'throughout': 13,\n",
       "         'smart': 66,\n",
       "         'get': 13,\n",
       "         'an': 13,\n",
       "         'value': 23,\n",
       "         'significantly': 13,\n",
       "         'extremely': 13,\n",
       "         'australia,': 13,\n",
       "         'internet': 13,\n",
       "         'between': 13,\n",
       "         'various': 13,\n",
       "         'ways': 13,\n",
       "         'working': 13,\n",
       "         'users': 13,\n",
       "         'already': 13,\n",
       "         'data.': 13,\n",
       "         'services': 23,\n",
       "         'which': 13,\n",
       "         'valuable': 13,\n",
       "         'real-world': 13,\n",
       "         'dubai,': 13,\n",
       "         'many': 13,\n",
       "         'the': 141,\n",
       "         'has': 23,\n",
       "         'implemented': 13,\n",
       "         'indicator': 13,\n",
       "         'increased': 13,\n",
       "         'in': 72,\n",
       "         'bulk': 13,\n",
       "         'its': 13,\n",
       "         'al.,': 13,\n",
       "         'mobile': 13,\n",
       "         'along': 13,\n",
       "         'people.': 13,\n",
       "         'with': 39,\n",
       "         'world': 13,\n",
       "         'applications,': 13,\n",
       "         'significant': 13,\n",
       "         'as': 46,\n",
       "         'definition': 13,\n",
       "         'cultural,': 13,\n",
       "         'deployment': 13,\n",
       "         'collection': 13,\n",
       "         'regarded': 13,\n",
       "         '2021).': 13,\n",
       "         'update': 13,\n",
       "         'and': 53,\n",
       "         'was': 13,\n",
       "         'real-time': 13,\n",
       "         'basis': 13,\n",
       "         'taken': 13,\n",
       "         'line': 13,\n",
       "         'devices': 13,\n",
       "         'for': 23,\n",
       "         'facets': 13})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5e03e-6090-4e4d-bba7-21060aa0c02d",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d5b832b-b8ad-44c6-880b-c08e33f0844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k: # num of sampling\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    \n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc40fb-c957-4f1a-8421-72778e09238e",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d02c8ec-f149-47bc-938e-918bbfde4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_batch  = torch.Tensor(input_batch)\n",
    "# target_batch = torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "789341b4-ab95-4aa5-ac87-b784868f1fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2e28cfb-2113-4a03-aa56-330f8348b954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c2081c6-5f04-405e-b6b4-77639cfd5830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[138,  98, 128],\n",
       "        [149, 102,  37]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_neg = 3\n",
    "negative_sampling(target_batch, unigram_table, num_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420f6d5-8438-48ec-a518-a70436312647",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de7c2837-4fea-4ec2-ac85-afda3985c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size) # out embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds    = -self.embedding_u(negative_words) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, k, 1]\n",
    "        \n",
    "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1)\n",
    "                \n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08e36e-8e4f-4565-aaa3-0ceb2ed9d440",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20e3d6a2-edaf-41e6-964f-94f969c2f8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "858a55ac-bc65-47e1-86fb-9db7c82fbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 2 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "model          = SkipgramNegSampling(len(vocab), embedding_size)\n",
    "num_neg        = 10 # num of negative sampling\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f25c854-32cd-48de-8534-699d4fd5b4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipgramNegSampling(\n",
       "  (embedding_v): Embedding(178, 2)\n",
       "  (embedding_u): Embedding(178, 2)\n",
       "  (logsigmoid): LogSigmoid()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05c32b6d-6bfc-4ed4-ba5a-6a6af7f84298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a8ff003-770b-472c-88f2-72371e8a2d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 10.828545 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 6.657749 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 10.185398 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 7.603157 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 7.236413 | time: 0m 0s\n",
      "Epoch: 6000 | cost: 7.895445 | time: 0m 0s\n",
      "Epoch: 7000 | cost: 7.785011 | time: 0m 0s\n",
      "Epoch: 8000 | cost: 7.051541 | time: 0m 0s\n",
      "Epoch: 9000 | cost: 7.138447 | time: 0m 0s\n",
      "Epoch: 10000 | cost: 6.980354 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    #input_batch: [batch_size, 1]\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    \n",
    "    #target_batch: [batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    \n",
    "    #negs_batch:   [batch_size, num_neg]\n",
    "    negs_batch = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    loss = model(input_batch, target_batch, negs_batch)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e9cb0-4a3f-4d4e-ad50-94436b19f9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
