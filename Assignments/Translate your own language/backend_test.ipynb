{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a3c623-44e3-47f3-b4b7-219b1079dbe9",
   "metadata": {},
   "source": [
    "# Test for Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d266f35-4705-405c-be1f-5a97f2f6ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7428c1-8d61-401e-a766-3b95e3d24752",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d665c9-7e94-4ab3-a377-b6a9bdb5115a",
   "metadata": {},
   "source": [
    "## 1) Loading Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d946f7-e8a9-416c-a2af-386e830dd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_transform = torch.load('vocab_transform.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416ba56-63cf-4b8d-a677-7ba109d4bd26",
   "metadata": {},
   "source": [
    "## 2) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22428bfd-1f0a-46b1-a87e-dbb2e3a81c5d",
   "metadata": {},
   "source": [
    "### 2.1) Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f3a4f5-b174-46b8-9e6b-891724fad0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=False)\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)        \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * 2]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63f184-17e3-4ca0-9ab1-d17dadca943d",
   "metadata": {},
   "source": [
    "### 2.2) Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4d3617-3b40-4b76-9f30-507c3864800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        self.W = nn.Linear(hid_dim,     hid_dim) #for decoder\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim) #for encoder outputs\n",
    "                \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #hidden = [batch size, src len, hid dim]\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.W(hidden) + self.U(encoder_outputs))\n",
    "        #energy = [batch size, src len, hid dim]\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        #use masked_fill_ if you want in-place\n",
    "        attention = attention.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841a419-15c5-4395-8b4c-c086cb63e338",
   "metadata": {},
   "source": [
    "### 2.3) Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f9657a-4e14-40c6-bb56-b5f41b3a3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted = [batch size, 1, hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted = [1, batch size, hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input = [1, batch size, (hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54817d2b-2c98-45a9-b116-e22bac74f415",
   "metadata": {},
   "source": [
    "### 2.3) Putting them together (become Seq2Seq!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f6ca03d-7fc1-4346-9104-b1f9e76832af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)  #permute so it's the same shape as attention\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, max_trg_len, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size     = src.shape[1]\n",
    "        trg_len        = max_trg_len\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #tensor to store attentiont outputs from decoder\n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input_ = torch.tensor([2])\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output    = [batch size, output dim]\n",
    "            #hidden    = [batch size, hid dim]\n",
    "            #attention = [batch size, src len]\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #place attentions in a tensor holding attention for each token\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input_ = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb374256-ef16-4570-8c60-019c3524dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'hi'\n",
    "TRG_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d61aed-4cbb-41fa-8346-6497fdd711f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b2eb72f-0915-4a90-92c8-1b2e7c0401e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX, SOS_IDX, EOS_IDX = 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193c6ce9-f65a-46be-87a2-3078c5b2ca17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(1768, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(1863, 256)\n",
       "    (gru): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=1863, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = Attention(hid_dim)\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "model = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babf0c6-5890-43c8-97c5-7f13353531ed",
   "metadata": {},
   "source": [
    "## 3) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70bc1f96-8d38-4bf4-bd1e-b8d7a51d62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6692dc25-a340-4d85-b33b-a2ffdafda34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06ef531d-26ba-43c8-8782-140b9650b290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 22:52:38 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2023-03-05 22:52:38 INFO: Use device: gpu\n",
      "2023-03-05 22:52:38 INFO: Loading: tokenize\n",
      "2023-03-05 22:52:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "hindi_tokenizer = stanza.Pipeline('hi', processors='tokenize', download_method=None)\n",
    "\n",
    "def tokenizeHindiSent(text):\n",
    "    doc = hindi_tokenizer(text)\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        hindi_tokens = [token.text for token in sentence.tokens]\n",
    "    return hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ae71a8-8ee6-41ec-8a0e-1cac5e495f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_md')\n",
    "token_transform[SRC_LANGUAGE] = tokenizeHindiSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28fd92f5-cc29-4242-944a-3137f632d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce2d947-431b-402b-a292-03a0e62db662",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample  = ('अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें', 'Give your application an accessibility workout')\n",
    "sample2 = ('निचले पटल के लिए डिफोल्ट प्लग-इन खाका', 'The default plugin layout for the bottom panel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8f11e-742d-4b34-af54-9ac297b35bd8",
   "metadata": {},
   "source": [
    "### Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d587eaa-500d-4a8c-a5d5-233aba5bae4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  443,  137,    7,  264, 1178,    8, 1165,  451,    3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24d72d91-0b16-4579-8eb6-3f8b57f08e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  879,  111,  297,   42,  288, 1330,    3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6d8585-73cf-421f-a2c5-85071e97c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(-1, 1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86be9e50-83dc-459d-98a0-ad4736e94ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4be85c6-3138-4ad6-8c75-7eb66442aaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([8, 1]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5744aa62-2113-405d-bcf1-8c33827258e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eec15d9d-12b9-4fed-a356-bb17fa02543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './models/Seq2SeqPackedAttention.pt' \n",
    "\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, text_length, trg_text.shape[0], 0) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "885c6d85-febf-4d90-99d6-8d2b94e0a4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 1863])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #trg_len, batch_size, trg_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06f5de39-5aaf-48ea-b08c-a80b2ac61872",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a03f4625-2c25-456b-8b57-94e623936bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1863])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97641822-4470-4c4e-9588-b93baa3d41fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1863])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7209d64a-e9d4-4b81-bd56-b436a2558d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2f96144-5ae4-4a9a-978d-98ab7442fa7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([879, 111, 297,  42,   0,  42,   0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54208a59-7913-4543-b2df-53c0f4c797cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6cf9031-513c-4699-b8ed-263c410b8d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give\n",
      "your\n",
      "application\n",
      "an\n",
      "<unk>\n",
      "an\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc88d1c-2fd7-4846-94af-665eebd1c43b",
   "metadata": {},
   "source": [
    "### Sample2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8600c66-5562-483e-b743-89382480ce76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   0, 315,   9,  22, 400, 184, 528,   3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample2[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caae2df7-596a-4535-b895-05671c43afcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,  43, 194,  59, 178,  14,   4, 422, 223,   3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample2[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "217d8653-25e2-4853-81b8-ff8c3b96c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(-1, 1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "722deb00-dc60-46b5-b6f1-84da827638ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67016138-670d-4913-b4b8-e19e657c586a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 1]), torch.Size([10, 1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf1c06d3-20b4-4a22-a67a-afc647bfb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e95eeb04-1055-45d2-9dc5-cf17355db498",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, text_length, trg_text.shape[0], 0) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49effbd2-3bad-4d70-8636-79990142384c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 1863])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #trg_len, batch_size, trg_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e13d25d-f4e2-40ce-a26d-50b3119905cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b675f34a-335c-4549-ad52-919ef2f79460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1863])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d247408-e029-43bd-9e66-d2b14ce13f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1863])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c95671e-1801-4e46-8534-ceadb4609e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b8fff15-7396-44b9-b4e1-447d7091baad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 43, 202, 115,  14,   4,  59,   5,   3,   3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10217292-4eaa-4113-b0a3-6612921ef99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa150824-101e-4578-b7ee-fa2c0f74a099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "compiler\n",
      "flags\n",
      "for\n",
      "the\n",
      "plugin\n",
      ".\n",
      "<eos>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "567950b2-0413-43d7-a31e-7149b1c1e09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 43, 202, 115,  14,   4,  59,   5,   3,   3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2e7a186-4dab-49ec-b1d9-78439300579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "if torch.Tensor([2]) == 2:\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d36504-ab85-4340-a604-549147a6824c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
