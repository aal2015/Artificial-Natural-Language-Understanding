{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77dee8eb-ffea-45e4-b7b9-652aab31a65e",
   "metadata": {},
   "source": [
    "https://www.bdmi.org/Book-Reading/Charlie-and-the-Chocolate-Factory.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284d666-8bdb-4ed6-aced-21413e63d982",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e942319-d0be-4751-850c-43e0287e21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_content = [\n",
    "\"1 Here Comes Charlie\",\n",
    "\"2 Mr Willy Wonka’s Factory\",\n",
    "\"3 Mr Wonka and the Indian Prince\",\n",
    "\"4 The Secret Workers\",\n",
    "\"5 The Golden Tickets\",\n",
    "\"6 The First Two Finders\",\n",
    "\"7 Charlie’s Birthday\",\n",
    "\"8 Two More Golden Tickets Found\",\n",
    "\"9 Grandpa Joe Takes a Gamble\",\n",
    "\"10 The Family Begins to Starve\",\n",
    "\"11 The Miracle\",\n",
    "\"12 What It Said on the Golden Ticket\",\n",
    "\"13 The Big Day Arrives\",\n",
    "\"14 Mr Willy Wonka\",\n",
    "\"15 The Chocolate Room\",\n",
    "\"16 The Oompa-Loompas\",\n",
    "\"17 Augustus Gloop Goes up the Pipe\",\n",
    "\"18 Down the Chocolate River\",\n",
    "\"19 The Inventing Room – Everlasting Gobstoppers and Hair Toffee\",\n",
    "\"20 The Great Gum Machine\",\n",
    "\"21 Good-bye Violet\",\n",
    "\"22 Along the Corridor\",\n",
    "\"23 Square Sweets That Look Round\",\n",
    "\"24 Veruca in the Nut Room\",\n",
    "\"25 The Great Glass Lift\",\n",
    "\"26 The Television-Chocolate Room\",\n",
    "\"27 Mike Teavee is Sent by Television\",\n",
    "\"28 Only Charlie Left\",\n",
    "\"29 The Other Children Go Home\",\n",
    "\"30 Charlie’s Chocolate Factory\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c71df8-9738-4b4f-8fe1-5c1a50c4f39b",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/3277503/how-to-read-a-file-line-by-line-into-a-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e21383-06ed-446f-a10b-389a603bf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chapters = [str(chapter) for chapter in range(31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45148b3b-5209-4e43-972e-04f77398760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Charlie and the Chocolate Factory.txt\", encoding=\"utf8\") as file:\n",
    "    book_lines = [line.rstrip('\\n') for line in file if line.rstrip('\\n') not in book_chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17956f9-af83-4d2e-9600-7491f1d2dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_lines = []\n",
    "skip_line = False\n",
    "\n",
    "with open(\"Charlie and the Chocolate Factory.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        if skip_line:\n",
    "            skip_line = False\n",
    "            continue\n",
    "        \n",
    "        line = line.rstrip('\\n')\n",
    "        if line in book_chapters:\n",
    "            skip_line = True\n",
    "            continue\n",
    "        \n",
    "        book_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9187bbe-e118-40cf-a741-9465298fe4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['These two very old people are the father and mother of Mr Bucket.',\n",
       " 'Their names are Grandpa Joe and Grandma Josephine.',\n",
       " 'And these two very old people are the father and mother of Mrs',\n",
       " 'Bucket. Their names are Grandpa George and Grandma Georgina.',\n",
       " 'This is Mr Bucket. This is Mrs Bucket.',\n",
       " 'Mr and Mrs Bucket have a small boy whose name is Charlie Bucket.',\n",
       " 'This is Charlie.',\n",
       " 'How d’you do? And how d’you do? And how d’you do again? He is',\n",
       " 'pleased to meet you.',\n",
       " 'The whole of this family – the six grown-ups (count them) and little',\n",
       " 'Charlie Bucket – live together in a small wooden house on the edge of a',\n",
       " 'great town.',\n",
       " 'The house wasn’t nearly large enough for so many people, and life',\n",
       " 'was extremely uncomfortable for them all. There were only two rooms',\n",
       " 'in the place altogether, and there was only one bed. The bed was given']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_lines[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d641d9-0f97-40bf-9347-eabc0b9c3267",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c27e3-516a-4a75-b445-0b77a63fd6b4",
   "metadata": {},
   "source": [
    "### Combining some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2743956e-c38f-4fd2-8af6-224fa5b62c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681d692d-350b-4023-8239-fda0f2227d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "lines = []\n",
    "current = \"\"\n",
    "combine_line_countdown = random.randint(1, 7) # Combine 'combine_line_countdown' number of sentences\n",
    "\n",
    "for line in book_lines:\n",
    "    current += \" \" + line\n",
    "    combine_line_countdown -= 1\n",
    "    \n",
    "    if combine_line_countdown == 0:\n",
    "        combine_line_countdown = random.randint(1, 3)\n",
    "        df.append(current.strip())\n",
    "        current = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7288ee0-cb12-4766-8dc9-ebdf2de9388d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['These two very old people are the father and mother of Mr Bucket. Their names are Grandpa Joe and Grandma Josephine. And these two very old people are the father and mother of Mrs Bucket. Their names are Grandpa George and Grandma Georgina. This is Mr Bucket. This is Mrs Bucket.',\n",
       " 'Mr and Mrs Bucket have a small boy whose name is Charlie Bucket.',\n",
       " 'This is Charlie. How d’you do? And how d’you do? And how d’you do again? He is pleased to meet you.',\n",
       " 'The whole of this family – the six grown-ups (count them) and little',\n",
       " 'Charlie Bucket – live together in a small wooden house on the edge of a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec7552e-7e3e-4173-b9a0-0986178a5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indices = [i for i in range(len(df))]\n",
    "random.shuffle(df_indices)\n",
    "\n",
    "val_split_index = int(0.2 * len(df_indices))\n",
    "train_idx       = df_indices[val_split_index:]\n",
    "val_idx         = df_indices[:val_split_index]\n",
    "\n",
    "def add_text_index(df, indices):\n",
    "    text_list = []\n",
    "    for idx in indices:\n",
    "        text_list.append(df[idx])\n",
    "    \n",
    "    return text_list\n",
    "\n",
    "# storing text in df_train for indices:\n",
    "df_train = add_text_index(df, train_idx)\n",
    "\n",
    "# storing text in df_val for indices:\n",
    "df_val = add_text_index(df, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4117a187-f772-4671-88ba-58045d4d37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_val) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9912a63b-b832-40fc-819f-7d924b7a7f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225, 306)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8924e5-f24e-49ee-bfb3-15f89380e42e",
   "metadata": {},
   "source": [
    "### Convert 'df_train' and 'df_val' into Dataset type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e691b36f-4cb9-4c27-8a7d-85fbdd605c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0e902d-6ecb-461c-9ee1-72ee693f28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [{\"text\": text} for text in df_train]\n",
    "df_train = Dataset.from_list(my_list)\n",
    "\n",
    "my_list = [{\"text\": text} for text in df_val]\n",
    "df_val  = Dataset.from_list(my_list)\n",
    "\n",
    "new_df = DatasetDict({\n",
    "    \"train\": df_train,\n",
    "    \"validation\": df_val\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd4f274c-9078-46a9-9f93-4a525308978e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1225\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 306\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b5154-6a96-464d-a3b9-7a0214e03540",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c977cbf-9299-40eb-a6f6-786ccf5107f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9feab31f-8156-4889-b8ce-f90ef3574305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 12\n",
      "Input chunk lengths: [10, 10, 10, 10, 10, 5, 10, 10, 10, 10, 10, 2]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "outputs = tokenizer(\n",
    "    new_df[\"train\"][:2][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=10,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2097a29-d5d6-45d9-8a14-647d215c1123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Sentence ----------------------\n",
      "‘But… but… but…’ shrieked Mrs Salt, ‘where does the great big pipe go to in the end?’ ‘Why, to the furnace, of course,’ Mr Wonka said calmly. ‘To the\n",
      "\n",
      "--------------------- Tokenization --------------------\n",
      "{'input_ids': [447, 246, 1537, 1399, 475, 1399, 475, 1399, 447, 247, 35064, 988, 276, 9074, 13754, 11, 564, 246, 3003, 857, 262, 1049, 1263, 12656, 467, 284, 287, 262, 886, 30, 447, 247, 564, 246, 5195, 11, 284, 262, 42227, 11, 286, 1781, 11, 447, 247, 1770, 23306, 4914, 531, 30180, 13, 564, 246, 2514, 262], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------- Sentence ----------------------\")\n",
    "print(new_df[\"train\"][0][\"text\"], end=\"\\n\\n\")\n",
    "\n",
    "print(\"--------------------- Tokenization --------------------\")\n",
    "print(tokenizer(new_df[\"train\"][0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de667099-7930-40d9-8b9b-e62146226f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72e8591aee34c5bb17b2524c8d48bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493fb63b15eb4ffbbdb836796d739100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3287\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 762\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 10\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = new_df.map(\n",
    "    tokenize, batched=True, remove_columns=new_df[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fde368-3bcb-400b-9daf-fa33e65f8a01",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4ba9a15-72f1-438f-8aab-1879dadd0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2b0329e-261a-4d40-bef7-ff964361ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9fc2e87-2297-4180-a77b-48c9d66b7b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36044a1f-bfb0-404f-b96c-b8e7a2853096",
   "metadata": {},
   "source": [
    "### Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e834cad9-d13f-4f21-888f-5db1bf95f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword has not single token: Wonka\n",
      "Keyword has not single token: chocolate\n",
      "Keyword has not single token: Chocolate\n",
      "Keyword has not single token: factory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"Charlie\",\n",
    "    \"Wonka\",\n",
    "    \"chocolate\",\n",
    "    \"Chocolate\",\n",
    "    \"Tickets\",\n",
    "    \"factory\",\n",
    "    \"Golden\",\n",
    "    \"Joe\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d4725-2420-4361-9036-05b279c2665a",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b9e9a3b-6f0d-40db-b67e-b5552f73031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False) #change to reduction=None\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88445094-b97c-434a-b16f-cd3949d2a889",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45f59427-e122-4dbc-b933-1b0f5a2ca099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True)\n",
    "eval_dataloader  = DataLoader(tokenized_datasets[\"validation\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfebfec-ea21-47b2-ac34-bb6c241e96c7",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d16ef5d-5484-4313-85da-68e88c7fb0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fba7117c-c011-4777-a464-007190897aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "            outputs.loss = outputs.loss.reshape(1)\n",
    "        losses.append(accelerator.gather(outputs.loss))        \n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a4e4d9a-121b-47a3-9201-485cb86e28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "728a6fe6-11b2-49f9-9939-641ef3c8762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1d5e9-9dee-4349-87ed-b3b97b9d6311",
   "metadata": {},
   "source": [
    "### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38e315d6-c36b-420e-8a60-fedbe5dea8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceb3728d-87ec-4e2f-91ea-926fbce60cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8448ef4-0e10-4c03-a832-5241e1be9cdc",
   "metadata": {},
   "source": [
    "### Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2afc6634-1fd6-45b5-8c27-b544f8aabccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b99d9fd443d42d9a4f11532239e49a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e46cf82a-20fb-44e3-9de7-3f9492f0d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Repository\n",
    "# from huggingface_hub import create_repo\n",
    "\n",
    "# create_repo(\"aal2015/Charlie-and-the-Chocolate_Factory-LM-mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c1f57f9-9c57-4370-8c11-0aaedc9fa932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aal2015/Charlie-and-the-Chocolate_Factory-LM-mode'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"Charlie-and-the-Chocolate_Factory-LM-mode\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08d60550-2820-4a83-9ee5-dffe64951125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30fde87e-063f-4e46-8ce6-466e5b851393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\OneDrive\\Documents\\AIT Thailand\\NLP\\04-Huggingface\\Charlie-and-the-Chocolate_Factory-LM-mode is already a clone of https://huggingface.co/aal2015/Charlie-and-the-Chocolate_Factory-LM-mode. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "output_dir = \"Charlie-and-the-Chocolate_Factory-LM-mode\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd83835-d3ee-438d-bd79-56a165a8092e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938ad098-c201-4bb4-8109-cddbe7f30d11",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ec2f88d-57ab-46c2-b837-e56b83bbae3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.048202514648438, 62830.9140625)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f17a557a-f01d-4928-ab7e-444a73c2fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Epoch:0----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c00aa468ee54b00b3feacbfb232c280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 0, 'loss/train': 93.81758880615234}\n",
      "{'steps': 0, 'loss/train': 88.20152282714844}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 0, 'loss/train': 88.10628509521484}\n",
      "{'steps': 0, 'loss/train': 88.19737243652344}\n",
      "{'steps': 0, 'loss/train': 88.0042724609375}\n",
      "{'steps': 0, 'loss/train': 88.34061431884766}\n",
      "{'steps': 0, 'loss/train': 91.35808563232422}\n",
      "{'steps': 0, 'loss/train': 88.39398193359375}\n",
      "{'steps': 1, 'loss/train': 88.24712371826172}\n",
      "{'steps': 1, 'loss/train': 90.87971496582031}\n",
      "{'steps': 1, 'loss/train': 87.82565307617188}\n",
      "{'steps': 1, 'loss/train': 88.11347961425781}\n",
      "{'steps': 1, 'loss/train': 87.80493927001953}\n",
      "{'steps': 1, 'loss/train': 88.02610778808594}\n",
      "{'steps': 1, 'loss/train': 88.13383483886719}\n",
      "{'steps': 1, 'loss/train': 91.17115783691406}\n",
      "{'loss/eval': 11.013158798217773, 'perplexity': 60667.21875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (10) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 2, 'loss/train': 87.94093322753906}\n",
      "{'steps': 2, 'loss/train': 87.9168701171875}\n",
      "{'steps': 2, 'loss/train': 90.57923889160156}\n",
      "{'steps': 2, 'loss/train': 92.84661865234375}\n",
      "{'steps': 2, 'loss/train': 90.68077087402344}\n",
      "{'steps': 2, 'loss/train': 88.14566802978516}\n",
      "{'steps': 2, 'loss/train': 88.23249053955078}\n",
      "{'steps': 2, 'loss/train': 87.72026062011719}\n",
      "{'steps': 3, 'loss/train': 87.86695861816406}\n",
      "{'steps': 3, 'loss/train': 87.18161010742188}\n",
      "{'steps': 3, 'loss/train': 87.67507934570312}\n",
      "{'steps': 3, 'loss/train': 90.54136657714844}\n",
      "{'steps': 3, 'loss/train': 87.14336395263672}\n",
      "{'steps': 3, 'loss/train': 87.50393676757812}\n",
      "{'steps': 3, 'loss/train': 87.45518493652344}\n",
      "{'steps': 3, 'loss/train': 87.66889953613281}\n",
      "{'loss/eval': 10.843338012695312, 'perplexity': 51191.97265625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (11) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 4, 'loss/train': 86.79505920410156}\n",
      "{'steps': 4, 'loss/train': 86.83560180664062}\n",
      "{'steps': 4, 'loss/train': 86.23213195800781}\n",
      "{'steps': 4, 'loss/train': 87.47259521484375}\n",
      "{'steps': 4, 'loss/train': 86.74269104003906}\n",
      "{'steps': 4, 'loss/train': 87.13511657714844}\n",
      "{'steps': 4, 'loss/train': 86.96968841552734}\n",
      "{'steps': 4, 'loss/train': 89.91542053222656}\n",
      "{'steps': 5, 'loss/train': 88.60795593261719}\n",
      "{'steps': 5, 'loss/train': 88.83068084716797}\n",
      "{'steps': 5, 'loss/train': 88.45805358886719}\n",
      "{'steps': 5, 'loss/train': 85.93077850341797}\n",
      "{'steps': 5, 'loss/train': 88.49138641357422}\n",
      "{'steps': 5, 'loss/train': 85.8282470703125}\n",
      "{'steps': 5, 'loss/train': 87.94056701660156}\n",
      "{'steps': 5, 'loss/train': 85.77117919921875}\n",
      "{'loss/eval': 10.566911697387695, 'perplexity': 38828.57421875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (12) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 6, 'loss/train': 87.84773254394531}\n",
      "{'steps': 6, 'loss/train': 84.50074768066406}\n",
      "{'steps': 6, 'loss/train': 85.05963897705078}\n",
      "{'steps': 6, 'loss/train': 85.04985046386719}\n",
      "{'steps': 6, 'loss/train': 84.66012573242188}\n",
      "{'steps': 6, 'loss/train': 87.82927703857422}\n",
      "{'steps': 6, 'loss/train': 85.00859069824219}\n",
      "{'steps': 6, 'loss/train': 84.30796813964844}\n",
      "{'steps': 7, 'loss/train': 86.50621032714844}\n",
      "{'steps': 7, 'loss/train': 83.3243637084961}\n",
      "{'steps': 7, 'loss/train': 83.60151672363281}\n",
      "{'steps': 7, 'loss/train': 83.17100524902344}\n",
      "{'steps': 7, 'loss/train': 83.08995056152344}\n",
      "{'steps': 7, 'loss/train': 84.00460815429688}\n",
      "{'steps': 7, 'loss/train': 86.28923797607422}\n",
      "{'steps': 7, 'loss/train': 84.02320861816406}\n",
      "{'loss/eval': 10.269451141357422, 'perplexity': 28838.0546875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (13) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 8, 'loss/train': 83.6007080078125}\n",
      "{'steps': 8, 'loss/train': 82.65158081054688}\n",
      "{'steps': 8, 'loss/train': 82.36135864257812}\n",
      "{'steps': 8, 'loss/train': 83.10652160644531}\n",
      "{'steps': 8, 'loss/train': 82.63813018798828}\n",
      "{'steps': 8, 'loss/train': 83.10719299316406}\n",
      "{'steps': 8, 'loss/train': 82.41522979736328}\n",
      "{'steps': 8, 'loss/train': 82.81314086914062}\n",
      "{'steps': 9, 'loss/train': 80.84086608886719}\n",
      "{'steps': 9, 'loss/train': 84.29546356201172}\n",
      "{'steps': 9, 'loss/train': 80.80013275146484}\n",
      "{'steps': 9, 'loss/train': 82.85252380371094}\n",
      "{'steps': 9, 'loss/train': 81.59756469726562}\n",
      "{'steps': 9, 'loss/train': 81.68305969238281}\n",
      "{'steps': 9, 'loss/train': 81.97419738769531}\n",
      "{'steps': 9, 'loss/train': 81.56845092773438}\n",
      "{'loss/eval': 10.02204418182373, 'perplexity': 22517.412109375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (14) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 10, 'loss/train': 80.42759704589844}\n",
      "{'steps': 10, 'loss/train': 80.64562225341797}\n",
      "{'steps': 10, 'loss/train': 80.36273193359375}\n",
      "{'steps': 10, 'loss/train': 80.22191619873047}\n",
      "{'steps': 10, 'loss/train': 80.57587432861328}\n",
      "{'steps': 10, 'loss/train': 78.68746948242188}\n",
      "{'steps': 10, 'loss/train': 80.6020278930664}\n",
      "{'steps': 10, 'loss/train': 79.41481018066406}\n",
      "{'steps': 11, 'loss/train': 81.92530822753906}\n",
      "{'steps': 11, 'loss/train': 79.90909576416016}\n",
      "{'steps': 11, 'loss/train': 78.67170715332031}\n",
      "{'steps': 11, 'loss/train': 81.94192504882812}\n",
      "{'steps': 11, 'loss/train': 80.15055847167969}\n",
      "{'steps': 11, 'loss/train': 80.26918029785156}\n",
      "{'steps': 11, 'loss/train': 79.3121337890625}\n",
      "{'steps': 11, 'loss/train': 79.34199523925781}\n",
      "{'loss/eval': 9.82398796081543, 'perplexity': 18471.568359375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (15) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 12, 'loss/train': 77.95243072509766}\n",
      "{'steps': 12, 'loss/train': 82.05929565429688}\n",
      "{'steps': 12, 'loss/train': 78.21273040771484}\n",
      "{'steps': 12, 'loss/train': 77.85730743408203}\n",
      "{'steps': 12, 'loss/train': 78.75857543945312}\n",
      "{'steps': 12, 'loss/train': 77.70838928222656}\n",
      "{'steps': 12, 'loss/train': 80.16534423828125}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 2\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"---------------------- Epoch:\" + str(epoch) + \"----------------------\")\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 1 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26d00f-471f-435d-9a13-539f0ff961e4",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bc02f05-faa0-49e1-b877-80fe9cb73483",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model aal2015/Charlie-and-the-Chocolate_Factory-LM-mode with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11248\\2994741280.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text-generation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"aal2015/Charlie-and-the-Chocolate_Factory-LM-mode\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;31m# Will load the correct model if possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m     \u001b[0mmodel_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m     framework, model = infer_framework_load_model(\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mmodel_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Could not load model {model} with any of the following classes: {class_tuple}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mframework\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tf\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TF\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model aal2015/Charlie-and-the-Chocolate_Factory-LM-mode with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel'>)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", max_length=100, pad_token_id=0, eos_token_id=0, model=\"aal2015/Charlie-and-the-Chocolate_Factory-LM-mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6242b-fc2e-4ccf-8f8f-21b1e91693d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
