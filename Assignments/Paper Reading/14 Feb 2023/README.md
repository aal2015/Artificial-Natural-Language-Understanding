| Topic | Attention Temperature Matters in Abstractive Summarization Distillation |
| ------------- | ------------- |
| Paper Link | https://aclanthology.org/2022.acl-long.11/ |
| Primary focus | Knowledge distillation and manipulating attention temperatures |
| Problem Addressed | Abstractive text summarization largely relies on large pre-trained seq-to-seq Transformer models. This makes it computationally expensive. |
