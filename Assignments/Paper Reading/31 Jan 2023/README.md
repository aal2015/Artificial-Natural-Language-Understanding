| Topic  | Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction |
| ------------- | ------------- |

| Aim  | Carry out an investigation of improving the GEC sequence tagging architecture by (1) ensembling multiple Transformer-based encoders, and (2) encoders in large configurations (more number of layers) |
|  | Perform knowledge distillation to train a single sequence tagger model  |

| Motivation | Improving GEC sequence tagging architecture |
| | Performing knowledge distillation since large models, especially due to ensembling, is expensive and convenient for deployment |

| Model | GECToR was the chosen sequence tagging model for the experiment. |
